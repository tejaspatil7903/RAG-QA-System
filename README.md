# ğŸ§  AI-Powered PDF Q&A System using LangChain & Mistral-7B

This project is a smart question-answering system that reads any PDF document and answers questions **only based on its content** using advanced Retrieval-Augmented Generation (RAG) techniques.

## ğŸš€ Features

- ğŸ” PDF document loader with chunking
- ğŸ§  HuggingFace embeddings + Chroma vector store
- ğŸ’¬ Mistral-7B instruct model via LangChain
- ğŸ¤– Chat-style prompting with context-grounded answers
- ğŸ§¾ PDF ingestion, vectorization, and retrieval
- âœ… Open-source, customizable, and easy to use

## ğŸ› ï¸ Tech Stack

- Python ğŸ
- LangChain ğŸ¦œ
- Mistral-7B via `langchain-openai`
- HuggingFace Embeddings
- Chroma Vector Store
- dotenv for environment management


## âš™ï¸ How It Works

1. Load and split a PDF using LangChainâ€™s `PyPDFLoader`.
2. Convert text chunks to vector embeddings using HuggingFace.
3. Store embeddings in a Chroma vector store.
4. Create a RAG chain using a Mistral-7B instruct model.
5. Ask a question â€” the model will only respond based on the document.

## ğŸ§ª Sample Question

> â“ *"What are encoder and decoder tasks?"*

ğŸ§  The system searches relevant chunks from the PDF and answers using the Mistral-7B model.

## ğŸ“¦ Installation

```bash
git clone https://github.com/tejaspatil7903/pdf-rag-qa.git
cd pdf-rag-qa
pip install -r requirements.txt
